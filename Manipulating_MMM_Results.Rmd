---
title: "Demystifying the Manipulation of Marketing Mix Modeling (MMM) Results"
output:
  html_document:
    df_print: paged
---

**tldr;** Ever wondered how MMM results could be skewed to reach any desired conclusion? Let me show you how. But there is no free lunch. The contributions for any variable can be changed but then the subsequent recommendation will go in the opposite direction.

Prelude: Picture this - a media executive once told me, "your model favors Facebook ads over TV, which contradicts our $10M TV ad advice to the client. Can you ‘adjust’ your model? We'll have eggs on our face if we present your current model's results. We'll lose the client [and you'll lose your job]." Sounds familiar? While this conversation is fictional, it resonates with many data scientists and MMM experts who have faced pressure to tweak their models.

**The Underlying Issue: **
In the realms of data science, machine learning, and AI, feature engineering is a common practice. In MMM, this translates to data transformations – think adstock for memory effect or power transformations for diminishing returns. I’ve explored these in [my earlier blog article](https://analyticsartist.wordpress.com/2014/01/31/adstock-rate-deriving-with-analytical-methods/). The problem arises when “closed-source” MMM vendors treat these transformations as hyperparameters, setting them before running the model. Their algorithms iterate over various hyperparameter values, generating thousands of models with decent R2 and p-values. This often leads to the cherry-picking of models.

**A Simple Example: **
Let's consider a simple case with two advertising campaigns. The first, TV, had a suboptimal ROI, while the second, Facebook, yielded impressive returns. Visualized data shows TV in red lagging behind Facebook's blue.

```{r simulate}
##<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Code is Poetry >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

suppressPackageStartupMessages({
  library(dplyr)
  library(tidyr)
  library(ggplot2)
  library(gridExtra)
  library(plotly)
  library(pander)
})

set.seed(1111)

weeks <- 156 # 3 years of weekly data
sales.base <- rep(1000, weeks) # The base sales is 1,000

# Simulate the first ad
facebook <- rep(0, weeks)

facebook[1:weeks] <- pmax(runif(weeks, 
                           runif(1, -100, -1), 
                           runif(1, 1, 1000)), 
                     0)

# Create parameters & impact
facebook_coef  <- runif(1, 100, 200)
facebook_power <- runif(1, 0, .5)

facebook_impact  <- facebook_coef * (facebook ^ facebook_power) + rnorm(weeks, 0, 10)

# Simulate the first ad
tv <- rep(0, weeks)

tv[1:weeks] <- pmax(runif(weeks, 
                           runif(1, -100, -1), 
                           runif(1, 1, 1000)), 
                     0)

# Create parameters & impact
tv_coef  <- runif(1,  10, 50)
tv_power <- runif(1, 0, .8)

tv_impact  <- tv_coef * (tv ^ tv_power) + rnorm(weeks, 0, 10)

# Error term
# error <- rnorm(weeks, 0, 200)

# Add the ads.impact to sales. We also include an error term
sales <- sales.base + facebook_impact + tv_impact

# Put everything into a dataframe and graph it.
modeling_data <- 
  data.frame(week            = seq(1, weeks),
             sales           = sales,
             facebook        = facebook,
             tv              = tv,
             facebook_impact = facebook_impact,
             tv_impact       = tv_impact)

simulation_parameters <-
  data.frame(variable     = "facebook",
             spend        = sum(facebook),
             coef         = facebook_coef,
             power        = facebook_power,
             contribution = sum(facebook_impact),
             ROI          = sum(facebook_impact)/sum(facebook)) %>%
  add_row(variable        ="tv",
             spend        = sum(tv),
             coef         = tv_coef,
             power        = tv_power,
             contribution = sum(tv_impact),
             ROI          = sum(tv_impact)/sum(tv))

pander(simulation_parameters)

# Convert the data to a long format
long_data <- modeling_data %>% 
  select(week, sales, facebook, tv) %>%
  gather(key = "category", value = "value", -week)

# Set color codes
facebook_color <- "#3b5998"
tv_color       <- "#FF6F61"
sales_color    <- "#000000"

# Create the time series plot
ggplot(long_data, aes(x = week, y = value, color = category)) +
  geom_line() +
  scale_color_manual(values = c("sales"    = sales_color, 
                                "facebook" = facebook_color, 
                                "tv"       = tv_color)) +
  labs(title = "Time Series Plot", 
       x     = "Week", 
       y     = "Value", 
       color = "Category") +
  theme_minimal()

# Reshape the data to a long format suitable for faceting
long_data <- rbind(
  data.frame(ad_variable = "facebook",
             ad_spend    = modeling_data$facebook,
             ad_impact   = modeling_data$facebook_impact),
  data.frame(ad_variable = "tv",
             ad_spend    = modeling_data$tv,
             ad_impact   = modeling_data$tv_impact))

# Create the scatter plots
ggplot(long_data, aes(x = ad_spend, y = ad_impact, color = ad_variable)) +
  geom_point() +
  geom_smooth() +
  scale_color_manual(values = c("facebook" = facebook_color, 
                                "tv"       = tv_color)) +
  #facet_wrap(~ad_variable, scales = "free") +
  labs(title = "Scatter Plots of Ad Spend vs. Impact", 
       x     = "Advertisement Spend", 
       y     = "Impact") +
  theme_minimal()
```

Here's the twist in a typical manual model: Data scientists create a series of power transformations (ranging from 0 to 1) for each variable. With 100 increments by 0.01 for both TV and Facebook, we end up examining 10,000 models.

In [my personal blog on MMM](https://analyticsartist.wordpress.com/2014/08/17/marketing-mix-modeling-explained-with-r/), I described an additive model like this:

sales = a0 + a1 * TV^p1 + a2 * FB^p2

The chart reveals that tweaking power transformation parameters can artificially inflate TV's ROI over Facebook's. Lower power values favor TV, while higher ones benefit Facebook. Intriguingly, both scenarios show significant R2 and p-values. Such manipulations usually happen behind the scenes, leaving marketers with a singular, possibly skewed result.

```{r}
# Create an empty data frame for the model results
results <- data.frame(model_num    = numeric(),
                      R_Squared    = numeric(),
                      ad_name      = character(),
                      spend        = numeric(),
                      power        = numeric(),
                      p_value      = numeric(),
                      contribution = numeric(),
                      ROI          = numeric())

power_sequence = seq(0.01, 1, 0.01)

model_counter <- 0

for (facebook_power in power_sequence) {
  for (tv_power in power_sequence) {
    # Transform
    facebook_transformed <- facebook ^ facebook_power
    tv_transformed       <- tv ^ tv_power
    
    # Model
    model <- lm(modeling_data$sales ~ facebook_transformed + tv_transformed)
    model_summary <- summary(model)
    
    # Contributions
    facebook_contribution <- as.numeric(model$coefficients["facebook_transformed"]) * sum(facebook_transformed)
    tv_contribution       <- as.numeric(model$coefficients["tv_transformed"])       * sum(tv_transformed)
    
    # Add Results to the model_results dataframe
    results <- results %>% 
      add_row(model_num    = model_counter,
              ad_name      = "facebook",
              spend        = sum(facebook),
              power        = facebook_power,
              p_value      = model_summary$coefficients["facebook_transformed", "Pr(>|t|)"],
              contribution = facebook_contribution,
              ROI          = facebook_contribution / sum(facebook)) %>%
      add_row(model_num    = model_counter,
              ad_name      = "tv",
              spend        = sum(tv),
              power        = tv_power,
              p_value      = model_summary$coefficients["tv_transformed", "Pr(>|t|)"],
              contribution = tv_contribution,
              ROI          = tv_contribution / sum(tv)) %>%
      add_row(model_num    = model_counter,
              R_Squared    = model_summary$r.squared)
    
    # Increment the Counter
    model_counter <- model_counter + 1
  }
}
```

```{r}
# Remove rows with NA in the power, ROI, or ad_name columns
results_clean <- results %>%
  select(c("power", "ROI", "ad_name")) %>%
  na.omit(results, cols = c("power", "ROI", "ad_name"))

# Create the scatter plot
ggplot(results_clean, aes(x = power, y = ROI, color = ad_name)) +
  geom_point(alpha=0.25) +
  scale_color_manual(values = c("facebook" = facebook_color, "tv" = tv_color)) +
  geom_hline(yintercept = 1.065 , linetype = "dashed", color = facebook_color, linewidth = 0.5) +
  geom_hline(yintercept = 0.8383, linetype = "dashed", color = tv_color      , linewidth = 0.5) +
  geom_text(aes(x = Inf, y = 1.065 , label = "True ROI", vjust = -0.5, hjust=1.05), color = facebook_color) +
  geom_text(aes(x = Inf, y = 0.8383, label = "True ROI", vjust = -0.5, hjust=1.05), color = tv_color      ) +
  labs(title = "Scatter Plot of ROIs by Power", 
       x     = "Power", 
       y     = "ROI") +
  theme_minimal()
```

**Introducing MMM Labs:** To combat these “model fixings” and manipulations, I'm launching [MMM Labs](https://mmmlabs.ai) – an open-source Marketing Mix Modeling initiative. The goal is to emulate the multiple model approach of platforms like [H2O.ai](https://h2o.ai) and [DataRobot](https://datarobot.com). With transparent, open-source code, we can ensure trustworthy results. It's about letting the best model prevail, not the most manipulated one.

Join the Conversation with your thoughts and experiences. Share and engage with this content to bring more transparency to MMM practices.